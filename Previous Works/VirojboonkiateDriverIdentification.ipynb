{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VirojboonkiateDriverIdentification.ipynb","provenance":[{"file_id":"1Bz8uL_0UxLwz9BKGanjWb-9UZT2tkpPP","timestamp":1611831298059},{"file_id":"1rUtW7B3Nnk3KJccT7xe1sJWRWrMY5Cc0","timestamp":1592186004208},{"file_id":"1sMFAzT7crn8kgzQTfHzZmAy9woC40ehO","timestamp":1562826948171}],"collapsed_sections":["DmdWRClDsZmQ","3IUuf3mwLIbC","G_0LKsFtsrWy","z09JsWeBs6gu","5QF2DKYXtO5v","UKbVXnSStShB","Nf_kVVkGthEJ"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DmdWRClDsZmQ"},"source":["# Driver Identification based on Virojboonkiate et al. 2019\n","Virojboonkiate, N., Chanakitkarnchok, A., Vateekul, P. and Rojviboonchai, K., 2019. Public transport driver identification system using histogram of acceleration data. Journal of Advanced Transportation, 2019.\n","\n","https://www.hindawi.com/journals/jat/2019/6372597/"]},{"cell_type":"markdown","metadata":{"id":"3IUuf3mwLIbC"},"source":["# Prepare"]},{"cell_type":"code","metadata":{"id":"S2ynvaresRxk"},"source":["import numpy as np\n","import pandas as pd\n","import random\n","import matplotlib.pyplot as plt\n","import os\n","import seaborn as sns\n","from scipy import stats\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics.classification import accuracy_score, recall_score, f1_score, precision_score\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn import preprocessing\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1-UBApotk3o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614732236548,"user_tz":-210,"elapsed":1226,"user":{"displayName":"Ruhallah Ahmadian","photoUrl":"","userId":"00240470440247106807"}},"outputId":"9c849a4f-57df-4ab8-88c9-0f5ff7e219b8"},"source":["# access to my google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"38467bFdsUTq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614732241698,"user_tz":-210,"elapsed":6368,"user":{"displayName":"Ruhallah Ahmadian","photoUrl":"","userId":"00240470440247106807"}},"outputId":"edb3a9be-aca4-46c9-b9d2-25340e36feaf"},"source":["#################### DOWNLOAD AND UNZIP FILE SAVED IN DRIVE ####################\n","\n","!pip install -U -q PyDrive\n","\n","\n","# HERE YOUR FILE ID ( GET IT WITH THE SHARING URL: https://drive.google.com/open?id=1MWZpk6SRmqBUGjwcdaEzoVDm7mUKOnci )\n","\n","zip_id = '1MWZpk6SRmqBUGjwcdaEzoVDm7mUKOnci'\n","\n","\n","\n","from pydrive.auth import GoogleAuth\n","\n","from pydrive.drive import GoogleDrive\n","\n","from google.colab import auth\n","\n","from oauth2client.client import GoogleCredentials\n","\n","import zipfile, os\n","\n","\n","# 1. Authenticate and create the PyDrive client.\n","\n","auth.authenticate_user()\n","\n","gauth = GoogleAuth()\n","\n","gauth.credentials = GoogleCredentials.get_application_default()\n","\n","drive = GoogleDrive(gauth)\n","\n","\n","\n","# DOWNLOAD ZIP\n","\n","print (\"Downloading zip file\")\n","\n","myzip = drive.CreateFile({'id': zip_id})\n","\n","myzip.GetContentFile('DrEftekhari.zip')\n","\n","\n","\n","# UNZIP ZIP\n","\n","print (\"Uncompressing zip file\")\n","\n","zip_ref = zipfile.ZipFile('DrEftekhari.zip', 'r')\n","\n","zip_ref.extractall()\n","\n","zip_ref.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n","    from google.appengine.api import memcache\n","ModuleNotFoundError: No module named 'google.appengine'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n","    from oauth2client.contrib.locked_file import LockedFile\n","ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n","    from oauth2client.locked_file import LockedFile\n","ModuleNotFoundError: No module named 'oauth2client.locked_file'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 42, in autodetect\n","    from . import file_cache\n","  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n","    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n","ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading zip file\n","Uncompressing zip file\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G_0LKsFtsrWy"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"Duvdw-3esvEc"},"source":["import os\n","from datetime import datetime, timedelta\n","import re\n","import pandas as pd\n","import numpy as np\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import math\n","from sklearn import preprocessing\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import classification_report\n","\n","\n","class Utils:\n","\n","    def __init__(self, sample_rate, data_split_ratio=0.7):\n","        self.all_features = ['x-accelerometer', 'y-accelerometer', 'z-accelerometer',\n","                             'GRAVITY X (m/s²)', 'GRAVITY Y (m/s²)', 'GRAVITY Z (m/s²)',\n","                             'LINEAR ACCELERATION X (m/s²)', 'LINEAR ACCELERATION Y (m/s²)',\n","                             'LINEAR ACCELERATION Z (m/s²)',\n","                             'x-gyroscope', 'y-gyroscope', 'z-gyroscope',\n","                             'LIGHT (lux)',\n","                             'MAGNETIC FIELD X (μT)', 'MAGNETIC FIELD Y (μT)', 'MAGNETIC FIELD Z (μT)',\n","                             'ORIENTATION Z (azimuth °)', 'ORIENTATION X (pitch °)', 'ORIENTATION Y (roll °)',\n","                             'LOCATION Latitude : ',\n","                             'LOCATION Longitude : ',\n","                             'LOCATION Altitude ( m)',\n","                             'LOCATION Altitude-google ( m)',\n","                             'LOCATION Speed ( Kmh)',\n","                             'LOCATION Accuracy ( m)',\n","                             'LOCATION ORIENTATION (°)',\n","                             'Satellites in range',\n","                             'Time since start in ms',\n","                             'timestamp']\n","        self.show_toolbar = True\n","        self.sample_rate = sample_rate\n","        self.data_split_ratio = data_split_ratio\n","\n","    def read_data(self, db_path_prefix, file_name, features, driver_i):\n","        db_path = db_path_prefix + file_name\n","        db_new_path = db_path_prefix + \"cleaned_by_acc/\"\n","\n","        if os.path.exists(db_new_path + file_name):\n","            data = pd.read_csv(db_new_path + file_name, low_memory=False)\n","            original_size = pd.read_csv(db_path, low_memory=False).shape[0]\n","            driving_size = data.shape[0]\n","            stay_size = original_size - driving_size\n","        else:\n","            data = pd.read_csv(db_path, low_memory=False)\n","            data.columns = self.all_features\n","\n","            original_size = data.shape[0]\n","\n","            def aggregate(point1, point2):\n","                return math.sqrt(math.pow(point1[0] - point2[0], 2) +\n","                                 math.pow(point1[1] - point2[1], 2) +\n","                                 math.pow(point1[2] - point2[2], 2))\n","\n","            staypoints = [0]\n","            points_acc = list(zip(data['x-accelerometer'], data['y-accelerometer'], data['z-accelerometer']))\n","            for j in range(0, len(points_acc) - 12):\n","                node = points_acc[j]\n","                add = True\n","                for j2 in range(j + 1, j + 12):\n","                    if aggregate(node, points_acc[j2]) > 0.5:\n","                        add = False\n","                if add:\n","                    staypoints.append(j)\n","\n","            if not staypoints.__contains__(len(points_acc) - 1):\n","                staypoints.append(len(points_acc) - 1)\n","\n","            stay_size = len(staypoints)\n","\n","            data = data.drop(index=data['x-accelerometer'][staypoints].index)\n","\n","            driving_size = data.shape[0]\n","\n","            if not os.path.exists(db_new_path):\n","                os.makedirs(db_new_path)\n","            data.to_csv(db_new_path + file_name, index=False)\n","\n","        missing_features = self.all_features.copy()\n","        for feature in features:\n","            missing_features.remove(feature)\n","        data = data.drop(columns=missing_features)\n","\n","        data = data.fillna(data.mean())\n","\n","        clean_driving_size = data.shape[0]\n","\n","        template = \"{0:20}{1:20}{2:20}{3:20}{4:20}\"\n","        if self.show_toolbar:\n","            self.show_toolbar = False\n","            print(template.format(\"driver_id: \", \"original_size: \", \"stay_size: \", \"driving_size: \",\n","                                  \"cleaned_driving_size: \"))\n","        print(template.format(str(driver_i),\n","                              str(timedelta(seconds=int(original_size / self.sample_rate))),\n","                              str(timedelta(seconds=int(stay_size / self.sample_rate))),\n","                              str(timedelta(seconds=int(driving_size / self.sample_rate))),\n","                              str(timedelta(seconds=int(clean_driving_size / self.sample_rate)))))\n","\n","        return self.split_to_train_test(data)\n","\n","    def split_to_train_test(self, data):\n","        return data[:int(len(data) * self.data_split_ratio)], data[int(len(data) * self.data_split_ratio):]\n","\n","    def save_result(self, saving_path, result, data, running_time):\n","        if not os.path.exists(saving_path):\n","            os.makedirs(saving_path)\n","        \n","        # Save to file\n","        with open(saving_path + 'statistics.txt', 'a') as f:\n","            f.write('\\n==========***==========\\n' +\n","                    datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\") +\n","                    '\\n' +\n","                    'running time :' + str(running_time.seconds) + \" seconds\" +\n","                    '\\n')\n","            f.write(str(data))\n","            f.write('\\n')\n","            f.write(str(result))\n","            f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z09JsWeBs6gu"},"source":["# Segmentation & Feature Extration"]},{"cell_type":"code","metadata":{"id":"DJwTzXNwEnzt"},"source":["class TransferToHistogram:\n","\n","    def __init__(self, window_size=90, window_ovrlap_size=45, num_bins=8):\n","        self.window_size = window_size\n","        self.overlapping = window_ovrlap_size\n","        self.num_bins = num_bins\n","\n","    def transfer(self, dataset, features, saving_histogram_charts=False):\n","        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n","        segments, labels = self.segment_signal(dataset, features)\n","        new_dataset = []\n","        print(\"making \" + str(len(segments)) + \" segments\")\n","        win = 0\n","        for segment, label in zip(segments, labels):\n","            row = []\n","            win += 1\n","            for feature_i in range(len(segment[1])):\n","                segment_f = segment[0:, [feature_i]]\n","                r1 = np.percentile(dataset[features[feature_i]],10)\n","                r2 = np.percentile(dataset[features[feature_i]],90)\n","                r = (r1 - r2 * 2, r1 + r2 * 2)\n","                count, bins = np.histogram(segment_f, bins=self.num_bins, range=r)\n","                his = count\n","                row = np.append(row, his, axis=0)\n","            row = np.append(row, [label], axis=0)\n","            new_dataset.append(row)\n","\n","        columns = []\n","        for feature in features:\n","            for i in range(self.num_bins):\n","                columns.append(feature + '-' + str(i))\n","        columns.append('id')\n","        df = pd.DataFrame(new_dataset, columns=columns)\n","        return df\n","\n","    def windows(self, data):\n","        start = 0\n","        while start < data.count():\n","            yield int(start), int(start + self.window_size)\n","            start += (self.window_size - self.overlapping)\n","\n","    def segment_signal(self, dataset, features):\n","        segments = np.empty((0, self.window_size, len(features)))\n","        labels = np.empty((0))\n","        for class_i in np.unique(dataset[\"id\"]):\n","            subset = dataset[dataset[\"id\"] == class_i]\n","            for (start, end) in self.windows(subset[\"id\"]):\n","                feature_slices = []\n","                for feature in features:\n","                    feature_slices.append(subset[feature][start:end])\n","                if len(feature_slices[0]) == self.window_size:\n","                    segments = np.vstack([segments, np.dstack(\n","                        [feature_slices[k] for k in range(len(feature_slices))])])\n","                    labels = np.append(labels, stats.mode(subset[\"id\"][start:end])[0][0])\n","        return segments, labels\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QF2DKYXtO5v"},"source":["# VARIABLES"]},{"cell_type":"code","metadata":{"id":"-cFSgYSrtR94"},"source":["def initialization(n_driver):\n","  global features\n","  features = ['x-accelerometer', 'y-accelerometer']\n","  global db_path_prefix\n","  db_path_prefix = ''\n","  global sample_rate\n","  sample_rate = 2\n","\n","  global window_size\n","  window_size = sample_rate * 60 * 15\n","  global overlapping\n","  overlapping = int(window_size * 0.75)\n","\n","  print(\"window size :\", str(timedelta(seconds=window_size / sample_rate)))\n","  print(\"overlap size :\", str(timedelta(seconds=overlapping / sample_rate)))\n","\n","  global utils\n","  utils = Utils(sample_rate=sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKbVXnSStShB"},"source":["#SEGMENTATION"]},{"cell_type":"code","metadata":{"id":"RXXZaU-VtVtv"},"source":["def read_data():\n","  global train_dataset\n","  train_dataset = pd.DataFrame()\n","  global test_dataset\n","  test_dataset = pd.DataFrame()\n","  for i in random.sample([i for i in range(201,211)], n_driver):\n","    train_temp_dataset, test_temp_dataset = utils.read_data(db_path_prefix, str(i) + '.1.csv', features, i)\n","    train_temp_dataset['id'] = i\n","    test_temp_dataset['id'] = i\n","    train_dataset = pd.concat([train_dataset, train_temp_dataset])\n","    test_dataset = pd.concat([test_dataset, test_temp_dataset])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eC40ncHOtZIv"},"source":["def replace_ids():\n","  replace_y_numbers = {k: v for v, k in enumerate(sorted(set(train_dataset.iloc[:, -1])))}\n","  train_dataset.iloc[:, -1] = train_dataset.iloc[:, -1].replace(replace_y_numbers)\n","  test_dataset.iloc[:, -1] = test_dataset.iloc[:, -1].replace(replace_y_numbers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzAPZad4taoL"},"source":["def feature_extraction():\n","  featureExtractor = TransferToHistogram(window_size=window_size, window_ovrlap_size=overlapping, num_bins=100)\n","\n","  global S_train\n","  S_train = featureExtractor.transfer(train_dataset, features)\n","  global S_test\n","  S_test = featureExtractor.transfer(test_dataset, features)\n","\n","  global SX_train\n","  SX_train = S_train.iloc[:, :-1]\n","  global y_train\n","  y_train = S_train.iloc[:, -1]\n","  global SX_test\n","  SX_test = S_test.iloc[:, :-1]\n","  global y_test\n","  y_test = S_test.iloc[:, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tX6FRgHb_Njx"},"source":["def Normalization(X_train, X_test):\n","  CX_train = X_train.copy()\n","  CX_test = X_test.copy()\n","\n","  scaler = preprocessing.Normalizer()\n","  scaler = scaler.fit(CX_train)\n","  NCX_train = scaler.transform(CX_train)\n","  NCX_test = scaler.transform(CX_test)\n","  \n","  return NCX_train, NCX_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcLsfmAd8Fsd"},"source":["def feature_normalization():\n","  global NSX_train\n","  global NSX_test\n","  NSX_train, NSX_test = Normalization(SX_train, SX_test)\n","  NSX_train = pd.DataFrame(NSX_train)\n","  NSX_test = pd.DataFrame(NSX_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nf_kVVkGthEJ"},"source":["#TRAINING"]},{"cell_type":"code","metadata":{"id":"jjhVu8WOqK5k"},"source":["def hyperparameter_tuning():\n","  param_grid = {\n","            'hidden_layer_sizes': [100],\n","            'activation': ['tanh', 'relu', 'logistic'],\n","            'solver': ['sgd', 'adam', 'lbfgs'],\n","            'alpha': 10.0 ** -np.arange(1, 4),\n","            'learning_rate': ['constant', \"invscaling\", 'adaptive'],\n","            'momentum': [0.2, 0.5, 0.7, 0.8],\n","            'random_state': [1, 2, 3, 4, 5, 6, 7, 8, 9]\n","        }\n","  grid_search = RandomizedSearchCV(MLPClassifier(), param_grid, random_state=0, cv=10,\n","                                         return_train_score=True, n_jobs=5)\n","  grid_search.fit(NSX_train, y_train)\n","  means = grid_search.cv_results_['mean_test_score']\n","  stds = grid_search.cv_results_['std_test_score']\n","  for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n","      print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n","  print(grid_search.best_params_)\n","  print(\"score:\", grid_search.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKHHOgBkwGxf"},"source":["def train_model():\n","  model = MLPClassifier(solver='lbfgs', random_state=2, momentum=0.5, learning_rate='invscaling', hidden_layer_sizes=100, alpha=0.001, activation='logistic')\n","  model_train = model.fit(NSX_train, y_train)\n","  tmp = model.predict(NSX_test)\n","  accuracy = accuracy_score(y_test, tmp)\n","  precision = precision_score(y_test, tmp, average='macro')\n","  recall = recall_score(y_test, tmp, average='macro')\n","  f1 = f1_score(y_test, tmp, average='macro')\n","  return accuracy, precision, recall, f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Y77UZ0Z4LOe"},"source":["def save_result(accuracy_list, recall_list, precision_list, f1_list, running_time):\n","  saving_path = '/content/drive/MyDrive/DriverIdentification/Driver Identification GSM+GAN/Log/Virojboonkiate/'\n","  data = {'window size':window_size / (60*sample_rate),'overlap':overlapping / window_size, \n","          'algoritm':'MLP', 'dataset':'eftekhari', 'drivers':n_driver, 'features':features}\n","  (accuracy_mean,accuracy_std) = (np.average(accuracy_list),np.std(accuracy_list))\n","  (recall_mean,recall_std) = (np.average(recall_list),np.std(recall_list))\n","  (precision_mean,precision_std) = (np.average(precision_list),np.std(precision_list))\n","  (f1_mean,f1_std) = (np.average(f1_list),np.std(f1_list))\n","  result = {\n","      'accuracy_mean':accuracy_mean,'accuracy_std':accuracy_std,\n","      'recall_mean':recall_mean,'recall_std':recall_std,\n","      'precision_mean':precision_mean,'precision_mean':precision_mean,\n","      'f1_mean':f1_mean,'f1_std':f1_std,\n","  }\n","  print('Mean Accuracy:{:.4f}({:.4f}) Mean Recall:{:.4f}({:.4f}) Mean Precision:{:.4f}({:.4f}) Mean F1:{:.4f}({:.4f})'.format(\n","      accuracy_mean,accuracy_std, recall_mean,recall_std, precision_mean,precision_std, f1_mean,f1_std))\n","  utils.save_result(saving_path=saving_path, result=result, data=data, running_time=running_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"08a3GARSb5Qh"},"source":["# RUN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KC8fj04EaxEl","executionInfo":{"status":"ok","timestamp":1614732297897,"user_tz":-210,"elapsed":62508,"user":{"displayName":"Ruhallah Ahmadian","photoUrl":"","userId":"00240470440247106807"}},"outputId":"e2f5cb4d-1101-4b09-864d-47a3ee2a4d92"},"source":["for n_driver in range(4,11):\n","  accuracy_list = []\n","  recall_list = []\n","  precision_list = []\n","  f1_list = []\n","  start = datetime.now()\n","  for i in range(10):\n","    initialization(n_driver)\n","    read_data()\n","    replace_ids()\n","    feature_extraction()\n","    feature_normalization()\n","    accuracy, precision, recall, f1 = train_model()\n","    accuracy_list.append(accuracy)\n","    precision_list.append(precision)\n","    recall_list.append(recall)\n","    f1_list.append(f1)\n","    print('Accuracy:{:.4f} Precision:{:.4f} Recall:{:.4f} F1:{:.4f}'.format(accuracy, precision, recall, f1))\n","  end = datetime.now()\n","  running_time = end - start\n","  save_result(accuracy_list, recall_list, precision_list, f1_list, running_time)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2958 Precision:0.2261 Recall:0.3182 F1:0.2393\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2254 Precision:0.2084 Recall:0.2150 F1:0.2024\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2676 Precision:0.2614 Recall:0.2891 F1:0.2248\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2394 Precision:0.2048 Recall:0.2200 F1:0.2077\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2676 Precision:0.2166 Recall:0.2550 F1:0.2253\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2254 Precision:0.2040 Recall:0.2250 F1:0.2069\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2676 Precision:0.2118 Recall:0.3000 F1:0.2211\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2817 Precision:0.2100 Recall:0.2500 F1:0.2167\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.2394 Precision:0.2132 Recall:0.2341 F1:0.2174\n","window size : 0:15:00\n","overlap size : 0:11:15\n","driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n","206                 6:44:40             4:46:42             1:57:58             1:57:58             \n","201                 4:19:09             2:38:41             1:40:28             1:40:28             \n","204                 8:39:25             6:50:00             1:49:25             1:49:25             \n","209                 6:34:13             4:20:44             2:13:29             2:13:29             \n","205                 9:38:07             8:10:24             1:27:43             1:27:43             \n","203                 8:24:28             5:41:16             2:43:11             2:43:11             \n","210                 6:05:37             3:14:38             2:50:59             2:50:59             \n","207                 12:13:19            9:10:35             3:02:44             3:02:44             \n","202                 6:53:36             4:59:01             1:54:35             1:54:35             \n","208                 8:00:58             5:54:09             2:06:49             2:06:49             \n","segmenting data with 109819 points\n","making 209 segments\n","segmenting data with 47069 points\n","making 71 segments\n","Accuracy:0.3099 Precision:0.2135 Recall:0.2700 F1:0.2226\n","Mean Accuracy:0.2620(0.0276) Mean Recall:0.2576(0.0339) Mean Precision:0.2170(0.0160) Mean F1:0.2184(0.0103)\n"],"name":"stdout"}]}]}