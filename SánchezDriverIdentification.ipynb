{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SánchezDriverIdentification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OluL38FKLf6b",
        "G_0LKsFtsrWy",
        "z09JsWeBs6gu",
        "5QF2DKYXtO5v",
        "UKbVXnSStShB",
        "Nf_kVVkGthEJ",
        "3Vx9Z5ZAyHyu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmdWRClDsZmQ"
      },
      "source": [
        "# Driver Identification based on Sánchez et al. 2020\n",
        "\n",
        "Sánchez, S.H., Pozo, R.F. and Gómez, L.A.H., 2020. Driver Identification and Verification From Smartphone Accelerometers Using Deep Neural Networks. IEEE Transactions on Intelligent Transportation Systems.\n",
        "\n",
        "https://ieeexplore.ieee.org/abstract/document/9145829"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OluL38FKLf6b"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ynvaresRxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c199a1-c1a3-417d-c000-1bf04f5b47dd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score, precision_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import preprocessing\n",
        "import keras\n",
        "from keras.layers import Dense, Input, Reshape, Flatten, BatchNormalization, LeakyReLU, GRU\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FhWkWONsvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b4e869-f025-4900-f8d0-25feffe903f9"
      },
      "source": [
        "# access to my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38467bFdsUTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8662f3-1844-490a-bf24-33fca5ed07b3"
      },
      "source": [
        "#################### DOWNLOAD AND UNZIP FILE SAVED IN DRIVE ####################\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "\n",
        "# HERE YOUR FILE ID ( GET IT WITH THE SHARING URL: https://drive.google.com/open?id=1MWZpk6SRmqBUGjwcdaEzoVDm7mUKOnci )\n",
        "\n",
        "zip_id = '1MWZpk6SRmqBUGjwcdaEzoVDm7mUKOnci'\n",
        "\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import zipfile, os\n",
        "\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "# DOWNLOAD ZIP\n",
        "\n",
        "print (\"Downloading zip file\")\n",
        "\n",
        "myzip = drive.CreateFile({'id': zip_id})\n",
        "\n",
        "myzip.GetContentFile('DrEftekhari.zip')\n",
        "\n",
        "\n",
        "\n",
        "# UNZIP ZIP\n",
        "\n",
        "print (\"Uncompressing zip file\")\n",
        "\n",
        "zip_ref = zipfile.ZipFile('DrEftekhari.zip', 'r')\n",
        "\n",
        "zip_ref.extractall()\n",
        "\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading zip file\n",
            "Uncompressing zip file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_0LKsFtsrWy"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duvdw-3esvEc"
      },
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "class Utils:\n",
        "\n",
        "    def __init__(self, sample_rate, train_segments, test_segments):\n",
        "        self.all_features = ['x-accelerometer', 'y-accelerometer', 'z-accelerometer',\n",
        "                             'GRAVITY X (m/s²)', 'GRAVITY Y (m/s²)', 'GRAVITY Z (m/s²)',\n",
        "                             'LINEAR ACCELERATION X (m/s²)', 'LINEAR ACCELERATION Y (m/s²)',\n",
        "                             'LINEAR ACCELERATION Z (m/s²)',\n",
        "                             'x-gyroscope', 'y-gyroscope', 'z-gyroscope',\n",
        "                             'LIGHT (lux)',\n",
        "                             'MAGNETIC FIELD X (μT)', 'MAGNETIC FIELD Y (μT)', 'MAGNETIC FIELD Z (μT)',\n",
        "                             'ORIENTATION Z (azimuth °)', 'ORIENTATION X (pitch °)', 'ORIENTATION Y (roll °)',\n",
        "                             'LOCATION Latitude : ',\n",
        "                             'LOCATION Longitude : ',\n",
        "                             'LOCATION Altitude ( m)',\n",
        "                             'LOCATION Altitude-google ( m)',\n",
        "                             'LOCATION Speed ( Kmh)',\n",
        "                             'LOCATION Accuracy ( m)',\n",
        "                             'LOCATION ORIENTATION (°)',\n",
        "                             'Satellites in range',\n",
        "                             'Time since start in ms',\n",
        "                             'timestamp']\n",
        "        self.show_toolbar = True\n",
        "        self.sample_rate = sample_rate\n",
        "        self.train_segments = train_segments\n",
        "        self.test_segments = test_segments\n",
        "\n",
        "    def read_data(self, db_path_prefix, file_name, features, driver_i):\n",
        "        db_path = db_path_prefix + file_name\n",
        "        db_new_path = db_path_prefix + \"cleaned_by_acc/\"\n",
        "\n",
        "        if os.path.exists(db_new_path + file_name):\n",
        "            data = pd.read_csv(db_new_path + file_name, low_memory=False)\n",
        "            original_size = pd.read_csv(db_path, low_memory=False).shape[0]\n",
        "            driving_size = data.shape[0]\n",
        "            stay_size = original_size - driving_size\n",
        "        else:\n",
        "            data = pd.read_csv(db_path, low_memory=False)\n",
        "            data.columns = self.all_features\n",
        "\n",
        "            original_size = data.shape[0]\n",
        "\n",
        "            def aggregate(point1, point2):\n",
        "                return math.sqrt(math.pow(point1[0] - point2[0], 2) +\n",
        "                                 math.pow(point1[1] - point2[1], 2) +\n",
        "                                 math.pow(point1[2] - point2[2], 2))\n",
        "\n",
        "            staypoints = [0]\n",
        "            points_acc = list(zip(data['x-accelerometer'], data['y-accelerometer'], data['z-accelerometer']))\n",
        "            for j in range(0, len(points_acc) - 12):\n",
        "                node = points_acc[j]\n",
        "                add = True\n",
        "                for j2 in range(j + 1, j + 12):\n",
        "                    if aggregate(node, points_acc[j2]) > 0.5:\n",
        "                        add = False\n",
        "                if add:\n",
        "                    staypoints.append(j)\n",
        "\n",
        "            if not staypoints.__contains__(len(points_acc) - 1):\n",
        "                staypoints.append(len(points_acc) - 1)\n",
        "\n",
        "            stay_size = len(staypoints)\n",
        "\n",
        "            data = data.drop(index=data['x-accelerometer'][staypoints].index)\n",
        "\n",
        "            driving_size = data.shape[0]\n",
        "\n",
        "            if not os.path.exists(db_new_path):\n",
        "                os.makedirs(db_new_path)\n",
        "            data.to_csv(db_new_path + file_name, index=False)\n",
        "        \n",
        "        if features.__contains__('gyroscope'):\n",
        "          data['gyroscope'] = np.linalg.norm(data[['x-gyroscope', 'y-gyroscope', 'z-gyroscope']].values, axis=1)\n",
        "        \n",
        "        missing_features = self.all_features.copy()\n",
        "        for feature in features:\n",
        "          if feature != 'gyroscope':\n",
        "            missing_features.remove(feature)\n",
        "        data = data.drop(columns=missing_features)\n",
        "\n",
        "        data = data.fillna(data.mean())\n",
        "        # data = data.dropna()\n",
        "\n",
        "        clean_driving_size = data.shape[0]\n",
        "\n",
        "        template = \"{0:20}{1:20}{2:20}{3:20}{4:20}\"\n",
        "        if self.show_toolbar:\n",
        "            self.show_toolbar = False\n",
        "            print(template.format(\"driver_id: \", \"original_size: \", \"stay_size: \", \"driving_size: \",\n",
        "                                  \"cleaned_driving_size: \"))\n",
        "        print(template.format(str(driver_i),\n",
        "                              str(timedelta(seconds=int(original_size / self.sample_rate))),\n",
        "                              str(timedelta(seconds=int(stay_size / self.sample_rate))),\n",
        "                              str(timedelta(seconds=int(driving_size / self.sample_rate))),\n",
        "                              str(timedelta(seconds=int(clean_driving_size / self.sample_rate)))))\n",
        "\n",
        "        return self.split_to_train_test(data)\n",
        "\n",
        "    def split_to_train_test(self, data):\n",
        "        flag = True\n",
        "        for (start, end) in self.train_segments:\n",
        "            if flag == True:\n",
        "              flag = False\n",
        "              train_data = data[start:end]\n",
        "            else:\n",
        "              train_data = pd.concat([data[start:end], train_data])\n",
        "        \n",
        "        flag = True\n",
        "        for (start, end) in self.test_segments:\n",
        "            if flag == True:\n",
        "              flag = False\n",
        "              test_data = data[start:end]\n",
        "            else:\n",
        "              test_data = pd.concat([data[start:end], test_data])\n",
        "        \n",
        "        return train_data, test_data\n",
        "\n",
        "    def save_result(self, saving_path, result, data, running_time):\n",
        "        if not os.path.exists(saving_path):\n",
        "            os.makedirs(saving_path)\n",
        "        \n",
        "        # Save to file\n",
        "        with open(saving_path + 'statistics.txt', 'a') as f:\n",
        "            f.write('\\n==========***==========\\n' +\n",
        "                    datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\") +\n",
        "                    '\\n' +\n",
        "                    'running time :' + str(running_time.seconds) + \" seconds\" +\n",
        "                    '\\n')\n",
        "            f.write(str(data))\n",
        "            f.write('\\n')\n",
        "            f.write(str(result))\n",
        "            f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z09JsWeBs6gu"
      },
      "source": [
        "# Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umSOvdXhFZ9c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Segmenter:\n",
        "\n",
        "    def __init__(self, window_size=90, window_overlap_size=45, sampling=2):\n",
        "        self.window_size = window_size\n",
        "        self.overlapping = window_overlap_size\n",
        "        self.sampling = sampling\n",
        "\n",
        "    def transfer(self, dataset, features):\n",
        "        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n",
        "        segments, labels = self.segment_signal(dataset, features)\n",
        "        new_dataset = []\n",
        "        print(\"making \" + str(len(segments)) + \" segments\")\n",
        "        for segment in segments:\n",
        "            row = []\n",
        "            for feature_i in range(len(features)):\n",
        "                row.append(segment[feature_i])\n",
        "            new_dataset.append(row)\n",
        "\n",
        "        new_dataset = np.array(new_dataset)\n",
        "        return new_dataset, labels\n",
        "\n",
        "    def windows(self, data):\n",
        "        start = 0\n",
        "        while start < data.count():\n",
        "            yield int(start), int(start + self.window_size)\n",
        "            start += (self.window_size - self.overlapping)\n",
        "\n",
        "    def segment_signal(self, dataset, features):\n",
        "        segments = []\n",
        "        labels = []\n",
        "        for class_i in np.unique(dataset[\"id\"]):\n",
        "            subset = dataset[dataset[\"id\"] == class_i]\n",
        "            for (start, end) in self.windows(subset[\"id\"]):\n",
        "                feature_slices = []\n",
        "                for feature in features:\n",
        "                    feature_slices.append(subset[feature][start:end].tolist())\n",
        "                if len(feature_slices[0]) == self.window_size:\n",
        "                    segments.append(feature_slices)\n",
        "                    labels.append(class_i)\n",
        "        return np.array(segments), np.array(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QF2DKYXtO5v"
      },
      "source": [
        "# VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cFSgYSrtR94"
      },
      "source": [
        "def initialization(n_driver, train_segments, test_segments):\n",
        "  global features\n",
        "  features = ['x-accelerometer', 'y-accelerometer', 'gyroscope']\n",
        "  # 'x-accelerometer','y-accelerometer','z-accelerometer','x-gyroscope','y-gyroscope','z-gyroscope', 'gyroscope'\n",
        "  global db_path_prefix\n",
        "  db_path_prefix = ''\n",
        "  global sample_rate\n",
        "  sample_rate = 2\n",
        "\n",
        "  global window_size\n",
        "  window_size = 224\n",
        "  global overlapping\n",
        "  overlapping = int(window_size * 0.25)\n",
        "\n",
        "  global utils\n",
        "  utils = Utils(sample_rate=sample_rate, train_segments=train_segments, test_segments=test_segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKbVXnSStShB"
      },
      "source": [
        "#SEGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXXZaU-VtVtv"
      },
      "source": [
        "def read_data():\n",
        "  global train_dataset\n",
        "  train_dataset = pd.DataFrame()\n",
        "  global test_dataset\n",
        "  test_dataset = pd.DataFrame()\n",
        "  for i in range(211 - n_driver, 211):\n",
        "    train_temp_dataset, test_temp_dataset = utils.read_data(db_path_prefix, str(i) + '.1.csv', features, i)\n",
        "    train_temp_dataset['id'] = i\n",
        "    test_temp_dataset['id'] = i\n",
        "    train_dataset = pd.concat([train_dataset, train_temp_dataset])\n",
        "    test_dataset = pd.concat([test_dataset, test_temp_dataset])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC40ncHOtZIv"
      },
      "source": [
        "def replace_ids():\n",
        "  replace_y_numbers = {k: v for v, k in enumerate(sorted(set(train_dataset.iloc[:, -1])))}\n",
        "  train_dataset.iloc[:, -1] = train_dataset.iloc[:, -1].replace(replace_y_numbers)\n",
        "  test_dataset.iloc[:, -1] = test_dataset.iloc[:, -1].replace(replace_y_numbers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMtymO_LDLBx"
      },
      "source": [
        "def Standardization(X_train, X_test):\n",
        "  CX_train = X_train.copy()\n",
        "  CX_test = X_test.copy()\n",
        "\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  scaler = scaler.fit(CX_train)\n",
        "  NCX_train = scaler.transform(CX_train)\n",
        "  NCX_test = scaler.transform(CX_test)\n",
        "\n",
        "  return NCX_train, NCX_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuQlLcreDTzT"
      },
      "source": [
        "def data_standardization():\n",
        "  global n_train_dataset\n",
        "  global n_test_dataset\n",
        "  n_train_dataset, n_test_dataset = Standardization(train_dataset.iloc[:, :-1], test_dataset.iloc[:, :-1])\n",
        "  n_train_dataset = pd.DataFrame(n_train_dataset, columns=features)\n",
        "  n_test_dataset = pd.DataFrame(n_test_dataset, columns=features)\n",
        "  n_train_dataset['id'] = train_dataset.iloc[:, -1].tolist()\n",
        "  n_test_dataset['id'] = test_dataset.iloc[:, -1].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzAPZad4taoL"
      },
      "source": [
        "def feature_extraction():\n",
        "  featureExtractor = Segmenter(window_size=window_size, window_overlap_size=overlapping)\n",
        "\n",
        "  global SX_train\n",
        "  global y_train\n",
        "  SX_train, y_train = featureExtractor.transfer(n_train_dataset, features)\n",
        "  global SX_test\n",
        "  global y_test\n",
        "  SX_test, y_test = featureExtractor.transfer(n_test_dataset, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf_kVVkGthEJ"
      },
      "source": [
        "#Model 1D to 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi70ZIRHtJyJ"
      },
      "source": [
        "class OnetoTwoDimensionNetwork():\n",
        "    def __init__(self, rows, cols, channels, classes):\n",
        "        # Input shape\n",
        "        self.img_rows = rows\n",
        "        self.img_cols = cols\n",
        "        self.channels = channels\n",
        "        self.classes = classes\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "\n",
        "        optimizer = Adam(0.001, 0.5)\n",
        "\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes signal as input and generates imgs\n",
        "        z = Input(shape=self.img_shape)\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        \n",
        "    def build_generator(self):\n",
        "      \n",
        "        model = Sequential()\n",
        "        \n",
        "        model.add(Conv1D(64, kernel_size=3, strides=1, activation='relu', padding=\"same\", kernel_regularizer='l2'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv1D(224, kernel_size=3, strides=1, activation='relu', padding=\"same\", kernel_regularizer='l2'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        img_shape = (224, 224)\n",
        "        \n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=img_shape))\n",
        "        model.add(Dense(self.classes, activation='softmax'))\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, X_train, y_train, X_valid, y_valid, batch_size=128):\n",
        "        logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
        "        tb = keras.callbacks.TensorBoard(log_dir=logdir, \n",
        "                                    histogram_freq=0, \n",
        "                                    batch_size = batch_size,\n",
        "                                    write_graph=True, \n",
        "                                    write_grads=False)\n",
        "\n",
        "        X_train = np.expand_dims(X_train, axis=2)\n",
        "        X_valid = np.expand_dims(X_valid, axis=2)\n",
        "\n",
        "        # Change the labels from categorical to one-hot encoding\n",
        "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype = np.int8)\n",
        "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype = np.int8)\n",
        "        \n",
        "        model_train =  self.combined.fit(X_train, y_train_onehot,\n",
        "                                              batch_size=batch_size,epochs=epochs,verbose=0,\n",
        "                                              validation_data=(X_valid, y_valid_onehot))\n",
        "        \n",
        "        return model_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vx9Z5ZAyHyu"
      },
      "source": [
        "# Training 1D to 2D model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0vpWqNhthho"
      },
      "source": [
        "def train_1d_to_2d():\n",
        "  global OtT1\n",
        "  global OtT2\n",
        "  global OtT3\n",
        "  OtT1 = OnetoTwoDimensionNetwork(rows=SX_train.shape[2], cols=1, channels=1, classes=n_driver)\n",
        "  OtT2 = OnetoTwoDimensionNetwork(rows=SX_train.shape[2], cols=1, channels=1, classes=n_driver)\n",
        "  OtT3 = OnetoTwoDimensionNetwork(rows=SX_train.shape[2], cols=1, channels=1, classes=n_driver)\n",
        "  \n",
        "  OtT1.train(epochs=20, X_train=SX_train[:,0,:], y_train=y_train, X_valid=SX_test[:,0,:], y_valid=y_test, batch_size=32)\n",
        "  OtT2.train(epochs=20, X_train=SX_train[:,1,:], y_train=y_train, X_valid=SX_test[:,1,:], y_valid=y_test, batch_size=32)\n",
        "  OtT3.train(epochs=20, X_train=SX_train[:,2,:], y_train=y_train, X_valid=SX_test[:,2,:], y_valid=y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLw7GcNaxK1d"
      },
      "source": [
        "def evaluate_1d_to_2d():\n",
        "  y_pred_probabilities1 = OtT1.combined.predict(np.expand_dims(SX_test[:,0,:], axis=2))\n",
        "  y_pred_probabilities2 = OtT2.combined.predict(np.expand_dims(SX_test[:,1,:], axis=2))\n",
        "  y_pred_probabilities3 = OtT3.combined.predict(np.expand_dims(SX_test[:,2,:], axis=2))\n",
        "  \n",
        "  y_pred_probabilities = np.add(y_pred_probabilities1, y_pred_probabilities2, y_pred_probabilities3)\n",
        "  \n",
        "  # decision_nb = 15\n",
        "  # decision_time = (decision_nb-1)*((window_size-overlapping)/sample_rate)+window_size/sample_rate\n",
        "  decision_time = 60 * 9\n",
        "  decision_nb = int((decision_time - (window_size / sample_rate)) / ((window_size - overlapping) / sample_rate)) + 1\n",
        "  print(\"decision time\", str(timedelta(seconds=decision_time)))\n",
        "\n",
        "  df = []\n",
        "  y_real = []\n",
        "\n",
        "  y_pred = np.asarray(pd.get_dummies(np.argmax(y_pred_probabilities, axis=1)), dtype = np.int8)\n",
        "  for class_i in np.unique(y_test):\n",
        "    subset = y_pred[np.where(y_test == class_i)]\n",
        "    for i in range(0,subset.shape[0],int(decision_nb*0.75)):\n",
        "      if i+decision_nb < subset.shape[0]:\n",
        "        row = np.zeros(y_pred.shape[1])\n",
        "        for j in range(decision_nb):\n",
        "          row += subset[i+j]\n",
        "        df.append(row)\n",
        "        y_real.append(class_i)\n",
        "  y_pred_labels = np.argmax(df, axis=1)\n",
        "  accuracy = accuracy_score(y_real, y_pred_labels)\n",
        "  precision = precision_score(y_real, y_pred_labels, average='macro')\n",
        "  recall = recall_score(y_real, y_pred_labels, average='macro')\n",
        "  f1 = f1_score(y_real, y_pred_labels, average='macro')\n",
        "  print('Accuracy:{:.4f} Recall:{:.4f} Precision:{:.4f} F1:{:.4f}'.format(accuracy, recall, precision, f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjVDHEMdkAjC"
      },
      "source": [
        "def generate_feature_maps():\n",
        "  global feature_maps1\n",
        "  global feature_maps2\n",
        "  global feature_maps3\n",
        "  \n",
        "  feature_maps1 = OtT1.generator.predict(np.expand_dims(SX_train[:,0,:], axis=2))\n",
        "  feature_maps2 = OtT2.generator.predict(np.expand_dims(SX_train[:,1,:], axis=2))\n",
        "  feature_maps3 = OtT3.generator.predict(np.expand_dims(SX_train[:,2,:], axis=2))\n",
        "  \n",
        "  global X_train_fm\n",
        "  X_train_fm = []\n",
        "  for i in range(SX_train.shape[0]):\n",
        "    row = []\n",
        "    for j in range(SX_train.shape[2]):\n",
        "      col = []\n",
        "      for k in range(SX_train.shape[2]):\n",
        "        cell = []\n",
        "        cell.append(feature_maps1[i][j][k])\n",
        "        cell.append(feature_maps1[i][j][k])\n",
        "        cell.append(feature_maps1[i][j][k])\n",
        "        col.append(cell)\n",
        "      row.append(col)\n",
        "    X_train_fm.append(row)\n",
        "  X_train_fm = np.array(X_train_fm)\n",
        "\n",
        "  feature_maps1 = OtT1.generator.predict(np.expand_dims(SX_test[:,0,:], axis=2))\n",
        "  feature_maps2 = OtT2.generator.predict(np.expand_dims(SX_test[:,1,:], axis=2))\n",
        "  feature_maps3 = OtT3.generator.predict(np.expand_dims(SX_test[:,2,:], axis=2))\n",
        "  \n",
        "  global X_test_fm\n",
        "  X_test_fm = []\n",
        "  for i in range(SX_test.shape[0]):\n",
        "    row = []\n",
        "    for j in range(SX_test.shape[2]):\n",
        "      col = []\n",
        "      for k in range(SX_test.shape[2]):\n",
        "        cell = []\n",
        "        cell.append(feature_maps1[i][j][k])\n",
        "        cell.append(feature_maps1[i][j][k])\n",
        "        cell.append(feature_maps1[i][j][k])\n",
        "        col.append(cell)\n",
        "      row.append(col)\n",
        "    X_test_fm.append(row)\n",
        "  X_test_fm = np.array(X_test_fm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1mS7Q2nI8A7"
      },
      "source": [
        "# DI Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C9orXWjJAZI"
      },
      "source": [
        "def train_model():\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "  x = base_model.output\n",
        "  x = Reshape(target_shape=((7, 7*2048)), name='reshape')(x)\n",
        "  x = GRU(1024, return_sequences=True, kernel_initializer='he_normal', name='gru1')(x)\n",
        "  x = GRU(10, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(x)\n",
        "  x = Flatten()(x) \n",
        "  predictions = Dense(n_driver, activation='softmax')(x)\n",
        "  model = Model(base_model.input, predictions)\n",
        "  \n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "  \n",
        "  # initiate Adam optimizer\n",
        "  opt = optimizer = Adam(0.001, 0.9)\n",
        "  \n",
        "  # Let's train the model using Adam\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  # model.summary()\n",
        "\n",
        "  y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype = np.int8)\n",
        "  y_valid_onehot = np.asarray(pd.get_dummies(y_test), dtype = np.int8)\n",
        "  history = model.fit(X_train_fm, y_train_onehot, batch_size=32, epochs=20, verbose=1,\n",
        "                    validation_data=(X_test_fm, y_valid_onehot))\n",
        "  \n",
        "  y_pred_prob_resnet = model.predict(X_test_fm)\n",
        "  \n",
        "  df = []\n",
        "  y_real = []\n",
        "  \n",
        "  decision_time = 60 * 9\n",
        "  decision_nb = int((decision_time - (window_size / sample_rate)) / ((window_size - overlapping) / sample_rate)) + 1\n",
        "\n",
        "  y_pred = np.asarray(pd.get_dummies(np.argmax(y_pred_prob_resnet, axis=1)), dtype = np.int8)\n",
        "  for class_i in np.unique(y_test):\n",
        "    subset = y_pred[np.where(y_test == class_i)]\n",
        "    for i in range(0,subset.shape[0],int(decision_nb*0.75)):\n",
        "      if i+decision_nb < subset.shape[0]:\n",
        "        row = np.zeros(y_pred.shape[1])\n",
        "        for j in range(decision_nb):\n",
        "          row += subset[i+j]\n",
        "        df.append(row)\n",
        "        y_real.append(class_i)\n",
        "  y_pred_labels = np.argmax(df, axis=1)\n",
        "  accuracy = accuracy_score(y_real, y_pred_labels)\n",
        "  precision = precision_score(y_real, y_pred_labels, average='macro')\n",
        "  recall = recall_score(y_real, y_pred_labels, average='macro')\n",
        "  f1 = f1_score(y_real, y_pred_labels, average='macro')\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krkNz-8cBZsd"
      },
      "source": [
        "def save_result(accuracy_list, recall_list, precision_list, f1_list, running_time):\n",
        "  saving_path = '/content/drive/MyDrive/DriverIdentification/Log/Sánchez/'\n",
        "  data = {'window size':window_size / (60*sample_rate),'overlap':overlapping / window_size, \n",
        "          'algoritm':'CNN-GRU', 'dataset':'eftekhari', 'drivers':n_driver, 'features':features}\n",
        "  (accuracy_mean,accuracy_std) = (np.average(accuracy_list),np.std(accuracy_list))\n",
        "  (recall_mean,recall_std) = (np.average(recall_list),np.std(recall_list))\n",
        "  (precision_mean,precision_std) = (np.average(precision_list),np.std(precision_list))\n",
        "  (f1_mean,f1_std) = (np.average(f1_list),np.std(f1_list))\n",
        "  result = {\n",
        "      'accuracy_mean':accuracy_mean,'accuracy_std':accuracy_std,\n",
        "      'recall_mean':recall_mean,'recall_std':recall_std,\n",
        "      'precision_mean':precision_mean,'precision_mean':precision_mean,\n",
        "      'f1_mean':f1_mean,'f1_std':f1_std,\n",
        "  }\n",
        "  print('Mean Accuracy:{:.4f}({:.4f}) Mean Recall:{:.4f}({:.4f}) Mean Precision:{:.4f}({:.4f}) Mean F1:{:.4f}({:.4f})'.format(\n",
        "      accuracy_mean,accuracy_std, recall_mean,recall_std, precision_mean,precision_std, f1_mean,f1_std))\n",
        "  utils.save_result(saving_path=saving_path, result=result, data=data, running_time=running_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBtxKDwFpNnw"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLR9MHyrBhs1",
        "outputId": "51804b3f-49a1-44f4-a236-21a2fd52ab0d"
      },
      "source": [
        "segments = np.arange(15)\n",
        "# np.random.shuffle(segments)\n",
        "segment_length = 2 * 60 * 5\n",
        "data_segments = []\n",
        "for i in range(15):\n",
        "    start = segments[i] * segment_length\n",
        "    end = segments[i] * segment_length + segment_length - 1\n",
        "    data_segments.append((start, end))\n",
        "\n",
        "for i in range(10):\n",
        "  print('train proportions:', data_segments[0:i] + data_segments[i+6:15])\n",
        "  print('test  proportions:', data_segments[i:i+6])\n",
        "  print(\"==========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train proportions: [(3600, 4199), (4200, 4799), (4800, 5399), (5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (4200, 4799), (4800, 5399), (5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599), (3600, 4199)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (4800, 5399), (5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599), (3600, 4199), (4200, 4799)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(1800, 2399), (2400, 2999), (3000, 3599), (3600, 4199), (4200, 4799), (4800, 5399)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(2400, 2999), (3000, 3599), (3600, 4199), (4200, 4799), (4800, 5399), (5400, 5999)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(3000, 3599), (3600, 4199), (4200, 4799), (4800, 5399), (5400, 5999), (6000, 6599)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(3600, 4199), (4200, 4799), (4800, 5399), (5400, 5999), (6000, 6599), (6600, 7199)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599), (3600, 4199), (7800, 8399), (8400, 8999)]\n",
            "test  proportions: [(4200, 4799), (4800, 5399), (5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599), (3600, 4199), (4200, 4799), (8400, 8999)]\n",
            "test  proportions: [(4800, 5399), (5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399)]\n",
            "==========================================\n",
            "train proportions: [(0, 599), (600, 1199), (1200, 1799), (1800, 2399), (2400, 2999), (3000, 3599), (3600, 4199), (4200, 4799), (4800, 5399)]\n",
            "test  proportions: [(5400, 5999), (6000, 6599), (6600, 7199), (7200, 7799), (7800, 8399), (8400, 8999)]\n",
            "==========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HNuxQ-YpOw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7b0830-ac10-4a91-bd31-b1f7570b5e21"
      },
      "source": [
        "for n_driver in range(4,5):\n",
        "  accuracy_list = []\n",
        "  recall_list = []\n",
        "  precision_list = []\n",
        "  f1_list = []\n",
        "  start = datetime.now()\n",
        "  for i in range(1):\n",
        "    train_segments = data_segments[0:i] + data_segments[i+6:15]\n",
        "    test_segments = data_segments[i:i+6]\n",
        "    initialization(n_driver, train_segments, test_segments)\n",
        "    read_data()\n",
        "    replace_ids()\n",
        "    data_standardization()\n",
        "    feature_extraction()\n",
        "    train_1d_to_2d()\n",
        "    evaluate_1d_to_2d()\n",
        "    generate_feature_maps()\n",
        "    accuracy, precision, recall, f1 = train_model()\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "    print('Accuracy:{:.4f} Precision:{:.4f} Recall:{:.4f} F1:{:.4f}'.format(accuracy, precision, recall, f1))\n",
        "  end = datetime.now()\n",
        "  running_time = end - start\n",
        "  save_result(accuracy_list, recall_list, precision_list, f1_list, running_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "driver_id:          original_size:      stay_size:          driving_size:       cleaned_driving_size: \n",
            "207                 12:13:19            9:10:35             3:02:44             3:02:44             \n",
            "208                 8:00:58             5:54:09             2:06:49             2:06:49             \n",
            "209                 6:34:13             4:20:44             2:13:29             2:13:29             \n",
            "210                 6:05:37             3:14:38             2:50:59             2:50:59             \n",
            "segmenting data with 21564 points\n",
            "making 124 segments\n",
            "segmenting data with 14376 points\n",
            "making 84 segments\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
            "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4e53a31840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "decision time 0:09:00\n",
            "Accuracy:0.7500 Recall:0.7500 Precision:0.5929 F1:0.6540\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4e6423a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 44s 10s/step - loss: 0.6190 - accuracy: 0.2327 - val_loss: 0.5527 - val_accuracy: 0.3333\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.5206 - accuracy: 0.5394 - val_loss: 0.5509 - val_accuracy: 0.3333\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.4943 - accuracy: 0.5715 - val_loss: 0.5722 - val_accuracy: 0.3333\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.4540 - accuracy: 0.6233 - val_loss: 0.5606 - val_accuracy: 0.2857\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 36s 10s/step - loss: 0.4122 - accuracy: 0.6835 - val_loss: 0.5720 - val_accuracy: 0.2738\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.3720 - accuracy: 0.7637 - val_loss: 0.5746 - val_accuracy: 0.2857\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 38s 11s/step - loss: 0.3485 - accuracy: 0.7701 - val_loss: 0.5833 - val_accuracy: 0.2500\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.3281 - accuracy: 0.8250 - val_loss: 0.5755 - val_accuracy: 0.2976\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.2923 - accuracy: 0.8883 - val_loss: 0.6023 - val_accuracy: 0.2976\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.2720 - accuracy: 0.8873 - val_loss: 0.5861 - val_accuracy: 0.2857\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.2352 - accuracy: 0.9334 - val_loss: 0.5918 - val_accuracy: 0.2857\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.2309 - accuracy: 0.9484 - val_loss: 0.6103 - val_accuracy: 0.3095\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1909 - accuracy: 0.9568 - val_loss: 0.6104 - val_accuracy: 0.2976\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1741 - accuracy: 0.9800 - val_loss: 0.6146 - val_accuracy: 0.2976\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1501 - accuracy: 1.0000 - val_loss: 0.6276 - val_accuracy: 0.3333\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1438 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 0.3452\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1440 - accuracy: 1.0000 - val_loss: 0.6534 - val_accuracy: 0.2857\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1239 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.3095\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1122 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 0.3214\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 35s 10s/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.2500\n",
            "Accuracy:0.0625 Precision:0.0227 Recall:0.0625 F1:0.0333\n",
            "Mean Accuracy:0.0625(0.0000) Mean Recall:0.0625(0.0000) Mean Precision:0.0227(0.0000) Mean F1:0.0333(0.0000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}