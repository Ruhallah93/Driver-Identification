{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"SánchezDriverIdentification.ipynb","provenance":[{"file_id":"1Bz8uL_0UxLwz9BKGanjWb-9UZT2tkpPP","timestamp":1611582400350},{"file_id":"1rUtW7B3Nnk3KJccT7xe1sJWRWrMY5Cc0","timestamp":1592186004208},{"file_id":"1sMFAzT7crn8kgzQTfHzZmAy9woC40ehO","timestamp":1562826948171}],"collapsed_sections":["OluL38FKLf6b","G_0LKsFtsrWy","z09JsWeBs6gu","5QF2DKYXtO5v","UKbVXnSStShB","Nf_kVVkGthEJ","3Vx9Z5ZAyHyu","h1mS7Q2nI8A7"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DmdWRClDsZmQ"},"source":["# Driver Identification based on Sánchez et al. 2020\n","\n","Sánchez, S.H., Pozo, R.F. and Gómez, L.A.H., 2020. Driver Identification and Verification From Smartphone Accelerometers Using Deep Neural Networks. IEEE Transactions on Intelligent Transportation Systems.\n","\n","https://ieeexplore.ieee.org/abstract/document/9145829"]},{"cell_type":"markdown","metadata":{"id":"OluL38FKLf6b"},"source":["# Prepare"]},{"cell_type":"code","metadata":{"id":"S2ynvaresRxk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614762859376,"user_tz":-210,"elapsed":8945,"user":{"displayName":"Ruhallah Ahmadian","photoUrl":"","userId":"00240470440247106807"}},"outputId":"a6d94a72-207a-409f-d41c-bb6d51a4a426"},"source":["import numpy as np\n","import pandas as pd\n","import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os\n","import seaborn as sns\n","from scipy import stats\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics.classification import accuracy_score, recall_score, f1_score, precision_score\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn import preprocessing\n","import keras\n","from keras.layers import Dense, Input, Reshape, Flatten, BatchNormalization, LeakyReLU, GRU\n","from keras.layers.convolutional import Conv1D\n","from keras.layers import GlobalAveragePooling2D\n","from keras.models import Model,Sequential\n","from keras.applications.resnet50 import ResNet50\n","from keras.preprocessing import image\n","from keras.applications.resnet50 import preprocess_input, decode_predictions\n","from keras.utils import to_categorical\n","from keras.optimizers import Adam\n","from termcolor import colored\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5FhWkWONsvH","executionInfo":{"status":"ok","timestamp":1614762859380,"user_tz":-210,"elapsed":8939,"user":{"displayName":"Ruhallah Ahmadian","photoUrl":"","userId":"00240470440247106807"}},"outputId":"2c557e81-ccc5-4739-ca4f-51bb0a3cc831"},"source":["# access to my google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38467bFdsUTq","executionInfo":{"status":"ok","timestamp":1614762873406,"user_tz":-210,"elapsed":22959,"user":{"displayName":"Ruhallah Ahmadian","photoUrl":"","userId":"00240470440247106807"}},"outputId":"378be699-6902-413c-a266-532d7abc8451"},"source":["#################### DOWNLOAD AND UNZIP FILE SAVED IN DRIVE ####################\n","\n","!pip install -U -q PyDrive\n","\n","\n","# HERE YOUR FILE ID ( GET IT WITH THE SHARING URL: https://drive.google.com/open?id=1MWZpk6SRmqBUGjwcdaEzoVDm7mUKOnci )\n","\n","zip_id = '1MWZpk6SRmqBUGjwcdaEzoVDm7mUKOnci'\n","\n","\n","\n","from pydrive.auth import GoogleAuth\n","\n","from pydrive.drive import GoogleDrive\n","\n","from google.colab import auth\n","\n","from oauth2client.client import GoogleCredentials\n","\n","import zipfile, os\n","\n","\n","# 1. Authenticate and create the PyDrive client.\n","\n","auth.authenticate_user()\n","\n","gauth = GoogleAuth()\n","\n","gauth.credentials = GoogleCredentials.get_application_default()\n","\n","drive = GoogleDrive(gauth)\n","\n","\n","\n","# DOWNLOAD ZIP\n","\n","print (\"Downloading zip file\")\n","\n","myzip = drive.CreateFile({'id': zip_id})\n","\n","myzip.GetContentFile('DrEftekhari.zip')\n","\n","\n","\n","# UNZIP ZIP\n","\n","print (\"Uncompressing zip file\")\n","\n","zip_ref = zipfile.ZipFile('DrEftekhari.zip', 'r')\n","\n","zip_ref.extractall()\n","\n","zip_ref.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading zip file\n","Uncompressing zip file\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G_0LKsFtsrWy"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"Duvdw-3esvEc"},"source":["import os\n","from datetime import datetime, timedelta\n","import re\n","import pandas as pd\n","import numpy as np\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import math\n","from sklearn import preprocessing\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import classification_report\n","\n","\n","class Utils:\n","\n","    def __init__(self, sample_rate, data_split_ratio=0.7):\n","        self.all_features = ['x-accelerometer', 'y-accelerometer', 'z-accelerometer',\n","                             'GRAVITY X (m/s²)', 'GRAVITY Y (m/s²)', 'GRAVITY Z (m/s²)',\n","                             'LINEAR ACCELERATION X (m/s²)', 'LINEAR ACCELERATION Y (m/s²)',\n","                             'LINEAR ACCELERATION Z (m/s²)',\n","                             'x-gyroscope', 'y-gyroscope', 'z-gyroscope',\n","                             'LIGHT (lux)',\n","                             'MAGNETIC FIELD X (μT)', 'MAGNETIC FIELD Y (μT)', 'MAGNETIC FIELD Z (μT)',\n","                             'ORIENTATION Z (azimuth °)', 'ORIENTATION X (pitch °)', 'ORIENTATION Y (roll °)',\n","                             'LOCATION Latitude : ',\n","                             'LOCATION Longitude : ',\n","                             'LOCATION Altitude ( m)',\n","                             'LOCATION Altitude-google ( m)',\n","                             'LOCATION Speed ( Kmh)',\n","                             'LOCATION Accuracy ( m)',\n","                             'LOCATION ORIENTATION (°)',\n","                             'Satellites in range',\n","                             'Time since start in ms',\n","                             'timestamp']\n","        self.show_toolbar = True\n","        self.sample_rate = sample_rate\n","        self.data_split_ratio = data_split_ratio\n","\n","    def read_data(self, db_path_prefix, file_name, features, driver_i):\n","        db_path = db_path_prefix + file_name\n","        db_new_path = db_path_prefix + \"cleaned_by_acc/\"\n","\n","        if os.path.exists(db_new_path + file_name):\n","            data = pd.read_csv(db_new_path + file_name, low_memory=False)\n","            original_size = pd.read_csv(db_path, low_memory=False).shape[0]\n","            driving_size = data.shape[0]\n","            stay_size = original_size - driving_size\n","        else:\n","            data = pd.read_csv(db_path, low_memory=False)\n","            data.columns = self.all_features\n","\n","            original_size = data.shape[0]\n","\n","            def aggregate(point1, point2):\n","                return math.sqrt(math.pow(point1[0] - point2[0], 2) +\n","                                 math.pow(point1[1] - point2[1], 2) +\n","                                 math.pow(point1[2] - point2[2], 2))\n","\n","            staypoints = [0]\n","            points_acc = list(zip(data['x-accelerometer'], data['y-accelerometer'], data['z-accelerometer']))\n","            for j in range(0, len(points_acc) - 12):\n","                node = points_acc[j]\n","                add = True\n","                for j2 in range(j + 1, j + 12):\n","                    if aggregate(node, points_acc[j2]) > 0.5:\n","                        add = False\n","                if add:\n","                    staypoints.append(j)\n","\n","            if not staypoints.__contains__(len(points_acc) - 1):\n","                staypoints.append(len(points_acc) - 1)\n","\n","            stay_size = len(staypoints)\n","\n","            data = data.drop(index=data['x-accelerometer'][staypoints].index)\n","\n","            driving_size = data.shape[0]\n","\n","            if not os.path.exists(db_new_path):\n","                os.makedirs(db_new_path)\n","            data.to_csv(db_new_path + file_name, index=False)\n","        \n","        if features.__contains__('gyroscope'):\n","          data['gyroscope'] = np.linalg.norm(data[['x-gyroscope', 'y-gyroscope', 'z-gyroscope']].values, axis=1)\n","        \n","        missing_features = self.all_features.copy()\n","        for feature in features:\n","          if feature != 'gyroscope':\n","            missing_features.remove(feature)\n","        data = data.drop(columns=missing_features)\n","\n","        data = data.fillna(data.mean())\n","\n","        clean_driving_size = data.shape[0]\n","\n","        template = \"{0:20}{1:20}{2:20}{3:20}{4:20}\"\n","        if self.show_toolbar:\n","            self.show_toolbar = False\n","            print(template.format(\"driver_id: \", \"original_size: \", \"stay_size: \", \"driving_size: \",\n","                                  \"cleaned_driving_size: \"))\n","        print(template.format(str(driver_i),\n","                              str(timedelta(seconds=int(original_size / self.sample_rate))),\n","                              str(timedelta(seconds=int(stay_size / self.sample_rate))),\n","                              str(timedelta(seconds=int(driving_size / self.sample_rate))),\n","                              str(timedelta(seconds=int(clean_driving_size / self.sample_rate)))))\n","\n","        return self.split_to_train_test(data)\n","\n","    def split_to_train_test(self, data):\n","        return data[:int(len(data) * self.data_split_ratio)], data[int(len(data) * self.data_split_ratio):]\n","\n","    def save_result(self, saving_path, result, data, running_time):\n","        if not os.path.exists(saving_path):\n","            os.makedirs(saving_path)\n","        \n","        # Save to file\n","        with open(saving_path + 'statistics.txt', 'a') as f:\n","            f.write('\\n==========***==========\\n' +\n","                    datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\") +\n","                    '\\n' +\n","                    'running time :' + str(running_time.seconds) + \" seconds\" +\n","                    '\\n')\n","            f.write(str(data))\n","            f.write('\\n')\n","            f.write(str(result))\n","            f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z09JsWeBs6gu"},"source":["# Segmentation"]},{"cell_type":"code","metadata":{"id":"umSOvdXhFZ9c"},"source":["import numpy as np\n","\n","\n","class Segmenter:\n","\n","    def __init__(self, window_size=90, window_overlap_size=45, sampling=2):\n","        self.window_size = window_size\n","        self.overlapping = window_overlap_size\n","        self.sampling = sampling\n","\n","    def transfer(self, dataset, features):\n","        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n","        segments, labels = self.segment_signal(dataset, features)\n","        new_dataset = []\n","        print(\"making \" + str(len(segments)) + \" segments\")\n","        for segment in segments:\n","            row = []\n","            for feature_i in range(len(features)):\n","                row.append(segment[feature_i])\n","            new_dataset.append(row)\n","\n","        new_dataset = np.array(new_dataset)\n","        return new_dataset, labels\n","\n","    def windows(self, data):\n","        start = 0\n","        while start < data.count():\n","            yield int(start), int(start + self.window_size)\n","            start += (self.window_size - self.overlapping)\n","\n","    def segment_signal(self, dataset, features):\n","        segments = []\n","        labels = []\n","        for class_i in np.unique(dataset[\"id\"]):\n","            subset = dataset[dataset[\"id\"] == class_i]\n","            for (start, end) in self.windows(subset[\"id\"]):\n","                feature_slices = []\n","                for feature in features:\n","                    feature_slices.append(subset[feature][start:end].tolist())\n","                if len(feature_slices[0]) == self.window_size:\n","                    segments.append(feature_slices)\n","                    labels.append(class_i)\n","        return np.array(segments), np.array(labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QF2DKYXtO5v"},"source":["# VARIABLES"]},{"cell_type":"code","metadata":{"id":"-cFSgYSrtR94"},"source":["def initialization(n_driver):\n","  global features\n","  features = ['x-accelerometer', 'y-accelerometer', 'gyroscope']\n","  global db_path_prefix\n","  db_path_prefix = ''\n","  global sample_rate\n","  sample_rate = 2\n","\n","  global window_size\n","  window_size = 224\n","  global overlapping\n","  overlapping = int(window_size * 0.75)\n","  global decision_time\n","  decision_time = 60 * 15\n","  global decision_nb\n","  decision_nb = int((decision_time - (window_size / sample_rate)) / ((window_size - overlapping) / sample_rate)) + 1\n","  global decision_nb_overalpping\n","  decision_nb_overalpping = int(decision_nb * (1 - 0.75))\n","  \n","  print(\"window size :\", str(timedelta(seconds=window_size / sample_rate)))\n","  print(\"overlap size :\", str(timedelta(seconds=overlapping / sample_rate)))\n","  print(\"decision time :\", str(timedelta(seconds=decision_time)))\n","  print(\"winows in decision :\", str(decision_nb))\n","\n","  global utils\n","  utils = Utils(sample_rate=sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKbVXnSStShB"},"source":["#SEGMENTATION"]},{"cell_type":"code","metadata":{"id":"RXXZaU-VtVtv"},"source":["def read_data():\n","  global train_dataset\n","  train_dataset = pd.DataFrame()\n","  global test_dataset\n","  test_dataset = pd.DataFrame()\n","  for i in random.sample([i for i in range(201,211)], n_driver):\n","    train_temp_dataset, test_temp_dataset = utils.read_data(db_path_prefix, str(i) + '.1.csv', features, i)\n","    train_temp_dataset['id'] = i\n","    test_temp_dataset['id'] = i\n","    train_dataset = pd.concat([train_dataset, train_temp_dataset])\n","    test_dataset = pd.concat([test_dataset, test_temp_dataset])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eC40ncHOtZIv"},"source":["def replace_ids():\n","  replace_y_numbers = {k: v for v, k in enumerate(sorted(set(train_dataset.iloc[:, -1])))}\n","  train_dataset.iloc[:, -1] = train_dataset.iloc[:, -1].replace(replace_y_numbers)\n","  test_dataset.iloc[:, -1] = test_dataset.iloc[:, -1].replace(replace_y_numbers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMtymO_LDLBx"},"source":["def Standardization(X_train, X_test):\n","  CX_train = X_train.copy()\n","  CX_test = X_test.copy()\n","\n","  scaler = preprocessing.Normalizer()\n","  scaler = scaler.fit(CX_train)\n","  NCX_train = scaler.transform(CX_train)\n","  NCX_test = scaler.transform(CX_test)\n","\n","  return NCX_train, NCX_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuQlLcreDTzT"},"source":["def data_standardization():\n","  global n_train_dataset\n","  global n_test_dataset\n","  n_train_dataset, n_test_dataset = Standardization(train_dataset.iloc[:, :-1], test_dataset.iloc[:, :-1])\n","  n_train_dataset = pd.DataFrame(n_train_dataset, columns=features)\n","  n_test_dataset = pd.DataFrame(n_test_dataset, columns=features)\n","  n_train_dataset['id'] = train_dataset.iloc[:, -1].tolist()\n","  n_test_dataset['id'] = test_dataset.iloc[:, -1].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzAPZad4taoL"},"source":["def feature_extraction():\n","  featureExtractor = Segmenter(window_size=window_size, window_overlap_size=overlapping)\n","\n","  global SX_train\n","  global y_train\n","  SX_train, y_train = featureExtractor.transfer(n_train_dataset, features)\n","  global SX_test\n","  global y_test\n","  SX_test, y_test = featureExtractor.transfer(n_test_dataset, features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nf_kVVkGthEJ"},"source":["#Model 1D to 2D"]},{"cell_type":"code","metadata":{"id":"hi70ZIRHtJyJ"},"source":["class OnetoTwoDimensionNetwork():\n","    def __init__(self, rows, cols, channels, classes):\n","        # Input shape\n","        self.img_rows = rows\n","        self.img_cols = cols\n","        self.channels = channels\n","        self.classes = classes\n","        self.img_shape = (self.img_rows, self.img_cols)\n","\n","        optimizer = Adam(0.001, 0.5)\n","\n","        self.discriminator = self.build_discriminator()\n","        self.generator = self.build_generator()\n","\n","        # The generator takes signal as input and generates imgs\n","        z = Input(shape=self.img_shape)\n","        img = self.generator(z)\n","\n","        # The discriminator takes generated images as input and determines validity\n","        validity = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","        \n","    def build_generator(self):\n","      \n","        model = Sequential()\n","        \n","        model.add(Conv1D(64, kernel_size=3, strides=1, activation='relu', padding=\"same\", kernel_regularizer='l2'))\n","        model.add(Conv1D(224, kernel_size=3, strides=1, activation='relu', padding=\"same\", kernel_regularizer='l2'))\n","\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def build_discriminator(self):\n","        img_shape = (224, 224)\n","        \n","        model = Sequential()\n","\n","        model.add(Flatten(input_shape=img_shape))\n","        model.add(Dense(self.classes, activation='softmax'))\n","\n","        img = Input(shape=img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, X_train, y_train, X_valid, y_valid, batch_size=128):\n","        logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n","        tb = keras.callbacks.TensorBoard(log_dir=logdir, \n","                                    histogram_freq=0, \n","                                    batch_size = batch_size,\n","                                    write_graph=True, \n","                                    write_grads=False)\n","\n","        X_train = np.expand_dims(X_train, axis=2)\n","        X_valid = np.expand_dims(X_valid, axis=2)\n","\n","        # Change the labels from categorical to one-hot encoding\n","        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype = np.int8)\n","        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype = np.int8)\n","        \n","        model_train =  self.combined.fit(X_train, y_train_onehot,\n","                                              batch_size=batch_size,epochs=epochs,verbose=0,\n","                                              validation_data=(X_valid, y_valid_onehot))\n","        \n","        return model_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Vx9Z5ZAyHyu"},"source":["# Training 1D to 2D model"]},{"cell_type":"code","metadata":{"id":"A0vpWqNhthho"},"source":["def train_1d_to_2d():\n","  global OtT1\n","  global OtT2\n","  global OtT3\n","  OtT1 = OnetoTwoDimensionNetwork(rows=SX_train.shape[2], cols=1, channels=1, classes=n_driver)\n","  OtT2 = OnetoTwoDimensionNetwork(rows=SX_train.shape[2], cols=1, channels=1, classes=n_driver)\n","  OtT3 = OnetoTwoDimensionNetwork(rows=SX_train.shape[2], cols=1, channels=1, classes=n_driver)\n","  \n","  OtT1.train(epochs=20, X_train=SX_train[:,0,:], y_train=y_train, X_valid=SX_test[:,0,:], y_valid=y_test, batch_size=32)\n","  OtT2.train(epochs=20, X_train=SX_train[:,1,:], y_train=y_train, X_valid=SX_test[:,1,:], y_valid=y_test, batch_size=32)\n","  OtT3.train(epochs=20, X_train=SX_train[:,2,:], y_train=y_train, X_valid=SX_test[:,2,:], y_valid=y_test, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLw7GcNaxK1d"},"source":["def evaluate_1d_to_2d():\n","  y_pred_probabilities1 = OtT1.combined.predict(np.expand_dims(SX_test[:,0,:], axis=2))\n","  y_pred_probabilities2 = OtT2.combined.predict(np.expand_dims(SX_test[:,1,:], axis=2))\n","  y_pred_probabilities3 = OtT3.combined.predict(np.expand_dims(SX_test[:,2,:], axis=2))\n","  \n","  y_pred_probabilities = np.add(y_pred_probabilities1, y_pred_probabilities2, y_pred_probabilities3)\n","\n","  df = []\n","  y_real = []\n","\n","  y_pred = np.asarray(pd.get_dummies(np.argmax(y_pred_probabilities, axis=1)), dtype = np.int8)\n","  for class_i in np.unique(y_test):\n","    subset = y_pred[np.where(y_test == class_i)]\n","    for i in range(0,subset.shape[0],decision_nb_overalpping):\n","      if i+decision_nb < subset.shape[0]:\n","        row = np.zeros(y_pred.shape[1])\n","        for j in range(decision_nb):\n","          row += subset[i+j]\n","        df.append(row)\n","        y_real.append(class_i)\n","  y_pred_labels = np.argmax(df, axis=1)\n","  accuracy = accuracy_score(y_real, y_pred_labels)\n","  precision = precision_score(y_real, y_pred_labels, average='macro')\n","  recall = recall_score(y_real, y_pred_labels, average='macro')\n","  f1 = f1_score(y_real, y_pred_labels, average='macro')\n","  print(colored('Accuracy:{:.4f} Recall:{:.4f} Precision:{:.4f} F1:{:.4f}'.format(accuracy, recall, precision, f1), \"green\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjVDHEMdkAjC"},"source":["def generate_feature_maps():\n","  global feature_maps1\n","  global feature_maps2\n","  global feature_maps3\n","  \n","  feature_maps1 = OtT1.generator.predict(np.expand_dims(SX_train[:,0,:], axis=2))\n","  feature_maps2 = OtT2.generator.predict(np.expand_dims(SX_train[:,1,:], axis=2))\n","  feature_maps3 = OtT3.generator.predict(np.expand_dims(SX_train[:,2,:], axis=2))\n","  \n","  global X_train_fm\n","  X_train_fm = []\n","  for i in range(SX_train.shape[0]):\n","    row = []\n","    for j in range(SX_train.shape[2]):\n","      col = []\n","      for k in range(SX_train.shape[2]):\n","        cell = []\n","        cell.append(feature_maps1[i][j][k])\n","        cell.append(feature_maps2[i][j][k])\n","        cell.append(feature_maps3[i][j][k])\n","        col.append(cell)\n","      row.append(col)\n","    X_train_fm.append(row)\n","  X_train_fm = np.array(X_train_fm)\n","\n","  feature_maps1 = OtT1.generator.predict(np.expand_dims(SX_test[:,0,:], axis=2))\n","  feature_maps2 = OtT2.generator.predict(np.expand_dims(SX_test[:,1,:], axis=2))\n","  feature_maps3 = OtT3.generator.predict(np.expand_dims(SX_test[:,2,:], axis=2))\n","  \n","  global X_test_fm\n","  X_test_fm = []\n","  for i in range(SX_test.shape[0]):\n","    row = []\n","    for j in range(SX_test.shape[2]):\n","      col = []\n","      for k in range(SX_test.shape[2]):\n","        cell = []\n","        cell.append(feature_maps1[i][j][k])\n","        cell.append(feature_maps2[i][j][k])\n","        cell.append(feature_maps3[i][j][k])\n","        col.append(cell)\n","      row.append(col)\n","    X_test_fm.append(row)\n","  X_test_fm = np.array(X_test_fm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1mS7Q2nI8A7"},"source":["# DI Model Training"]},{"cell_type":"code","metadata":{"id":"4C9orXWjJAZI"},"source":["def train_model():\n","  base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n","  for layer in base_model.layers:\n","    layer.trainable = False\n","  \n","  x = base_model.output\n","  x = Reshape(target_shape=((7, 7*2048)), name='reshape')(x)\n","  x = GRU(512, return_sequences=True, kernel_initializer='he_normal', name='gru1')(x)\n","  x = GRU(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(x)\n","  x = Flatten()(x) \n","  predictions = Dense(n_driver, activation='softmax')(x)\n","  model = Model(base_model.input, predictions)\n","  \n","  # this is the model we will train\n","  model = Model(inputs=base_model.input, outputs=predictions)\n","  \n","  # initiate Adam optimizer\n","  opt = optimizer = Adam(0.001, 0.5)\n","  \n","  # Let's train the model using Adam\n","  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","  # model.summary()\n","\n","  y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype = np.int8)\n","  y_valid_onehot = np.asarray(pd.get_dummies(y_test), dtype = np.int8)\n","  history = model.fit(X_train_fm, y_train_onehot, batch_size=32, epochs=20, verbose=1,\n","                    validation_data=(X_test_fm, y_valid_onehot))\n","  \n","  y_pred_prob_resnet = model.predict(X_test_fm)\n","  \n","  df = []\n","  y_real = []\n","  \n","  y_pred = np.asarray(pd.get_dummies(np.argmax(y_pred_prob_resnet, axis=1)), dtype = np.int8)\n","  for class_i in np.unique(y_test):\n","    subset = y_pred[np.where(y_test == class_i)]\n","    for i in range(0,subset.shape[0],decision_nb_overalpping):\n","      if i+decision_nb < subset.shape[0]:\n","        row = np.zeros(y_pred.shape[1])\n","        for j in range(decision_nb):\n","          row += subset[i+j]\n","        df.append(row)\n","        y_real.append(class_i)\n","  y_pred_labels = np.argmax(df, axis=1)\n","  accuracy = accuracy_score(y_real, y_pred_labels)\n","  precision = precision_score(y_real, y_pred_labels, average='macro')\n","  recall = recall_score(y_real, y_pred_labels, average='macro')\n","  f1 = f1_score(y_real, y_pred_labels, average='macro')\n","  return accuracy, precision, recall, f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krkNz-8cBZsd"},"source":["def save_result(accuracy_list, recall_list, precision_list, f1_list, running_time):\n","  saving_path = '/content/drive/MyDrive/DriverIdentification/Driver Identification GSM+GAN/Log/Sánchez/'\n","  data = {'window size':window_size / (60*sample_rate),'overlap':overlapping / window_size, \n","          'algoritm':'CNN-GRU', 'dataset':'eftekhari', 'drivers':n_driver, 'features':features}\n","  (accuracy_mean,accuracy_std) = (np.average(accuracy_list),np.std(accuracy_list))\n","  (recall_mean,recall_std) = (np.average(recall_list),np.std(recall_list))\n","  (precision_mean,precision_std) = (np.average(precision_list),np.std(precision_list))\n","  (f1_mean,f1_std) = (np.average(f1_list),np.std(f1_list))\n","  result = {\n","      'accuracy_mean':accuracy_mean,'accuracy_std':accuracy_std,\n","      'recall_mean':recall_mean,'recall_std':recall_std,\n","      'precision_mean':precision_mean,'precision_mean':precision_mean,\n","      'f1_mean':f1_mean,'f1_std':f1_std,\n","  }\n","  print('Mean Accuracy:{:.4f}({:.4f}) Mean Recall:{:.4f}({:.4f}) Mean Precision:{:.4f}({:.4f}) Mean F1:{:.4f}({:.4f})'.format(\n","      accuracy_mean,accuracy_std, recall_mean,recall_std, precision_mean,precision_std, f1_mean,f1_std))\n","  utils.save_result(saving_path=saving_path, result=result, data=data, running_time=running_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBtxKDwFpNnw"},"source":["# RUN"]},{"cell_type":"code","metadata":{"id":"2HNuxQ-YpOw4"},"source":["for n_driver in range(5,11):\n","  accuracy_list = []\n","  recall_list = []\n","  precision_list = []\n","  f1_list = []\n","  start = datetime.now()\n","  for i in range(10):\n","    initialization(n_driver)\n","    print(colored(\"Read Data\", \"blue\"))\n","    read_data()\n","    replace_ids()\n","    print(colored(\"Standardization\", \"blue\"))\n","    data_standardization()\n","    print(colored(\"Segmentation\", \"blue\"))\n","    feature_extraction()\n","    print(colored(\"Train 1D to 2D Model\", \"red\"))\n","    train_1d_to_2d()\n","    print(colored(\"Evaluation\", \"red\"))\n","    evaluate_1d_to_2d()\n","    print(colored(\"Generate Feature Maps\", \"red\"))\n","    generate_feature_maps()\n","    print(colored(\"Train Driver Identification Model\", \"yellow\"))\n","    accuracy, precision, recall, f1 = train_model()\n","    accuracy_list.append(accuracy)\n","    precision_list.append(precision)\n","    recall_list.append(recall)\n","    f1_list.append(f1)\n","    print(colored(\"Evaluation\", \"yellow\"))\n","    print(colored('Accuracy:{:.4f} Precision:{:.4f} Recall:{:.4f} F1:{:.4f}'.format(accuracy, precision, recall, f1), \"green\"))\n","  end = datetime.now()\n","  running_time = end - start\n","  save_result(accuracy_list, recall_list, precision_list, f1_list, running_time)"],"execution_count":null,"outputs":[]}]}